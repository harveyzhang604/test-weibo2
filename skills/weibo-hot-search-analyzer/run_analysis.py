#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¾®åšçƒ­æœäº§å“åˆ›æ„åˆ†æ - å®Œæ•´è¿è¡Œè„šæœ¬ v2.0
æ•´åˆæ•°æ®è·å–ã€æ™ºèƒ½åˆ†æã€æŠ¥å‘Šç”Ÿæˆçš„å®Œæ•´æµç¨‹
"""

import json
import requests
import os
import sys
from datetime import datetime
from jinja2 import Environment, FileSystemLoader
import argparse
import logging

# è§£å†³Windowsæ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

# å¯¼å…¥æ™ºèƒ½åˆ†æå™¨
from smart_analyzer import SmartAnalyzer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class WeiboHotSearchPipeline:
    """å¾®åšçƒ­æœåˆ†æå®Œæ•´æµç¨‹"""

    def __init__(self, api_key: str = None, output_prefix: str = 'weibo_analysis'):
        """
        åˆå§‹åŒ–æµç¨‹

        Args:
            api_key: å¤©è¡Œæ•°æ®APIå¯†é’¥
            output_prefix: è¾“å‡ºæ–‡ä»¶å‰ç¼€
        """
        self.api_key = api_key or '65f9a968f0869a2d63564093fed9d911'
        self.output_prefix = output_prefix
        self.base_dir = os.path.dirname(os.path.abspath(__file__))

        # åˆå§‹åŒ–åˆ†æå™¨
        self.analyzer = SmartAnalyzer(self.api_key)

    def fetch_hot_search(self) -> list:
        """è·å–å¾®åšçƒ­æœæ•°æ®"""
        url = f'https://apis.tianapi.com/weibohot/index?key={self.api_key}'

        logger.info("æ­£åœ¨è·å–å¾®åšçƒ­æœæ•°æ®...")

        try:
            response = requests.get(url, timeout=15)
            response.encoding = 'utf-8'
            data = response.json()

            if data.get('code') == 200:
                result_list = data.get('result', {}).get('list', [])
                logger.info(f"æˆåŠŸè·å– {len(result_list)} æ¡çƒ­æœæ•°æ®")

                # è½¬æ¢æ•°æ®æ ¼å¼
                hot_search_list = []
                for i, item in enumerate(result_list):
                    hot_num = item.get('hotnum', '0')
                    try:
                        if isinstance(hot_num, str):
                            hot_value = int(hot_num.replace(',', '').strip())
                        else:
                            hot_value = int(hot_num)
                    except:
                        hot_value = 0

                    hot_search_list.append({
                        'title': item.get('hotword', ''),
                        'heat': hot_value,
                        'tags': item.get('hottag', '').strip() if item.get('hottag') else '',
                        'rank': i + 1
                    })

                return hot_search_list
            else:
                logger.error(f"APIè¯·æ±‚å¤±è´¥: {data}")
                return []

        except Exception as e:
            logger.error(f"è·å–çƒ­æœæ•°æ®å¤±è´¥: {e}")
            return []

    def save_raw_data(self, data: list, filename: str):
        """ä¿å­˜åŸå§‹æ•°æ®"""
        output_data = {
            'fetch_time': datetime.now().isoformat(),
            'total_count': len(data),
            'data': data
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        logger.info(f"åŸå§‹æ•°æ®å·²ä¿å­˜åˆ°: {filename}")

    def generate_html_report(self, analysis_data: dict, output_file: str):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        template_path = os.path.join(self.base_dir, 'report_template_v2.html')

        env = Environment(loader=FileSystemLoader(self.base_dir))
        template = env.get_template('report_template_v2.html')

        # å‡†å¤‡æ¨¡æ¿æ•°æ®
        template_data = {
            'date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M'),
            'total_topics': analysis_data.get('total_topics', 0),
            'excellent_count': analysis_data.get('excellent_count', 0),
            'good_count': analysis_data.get('good_count', 0),
            'avg_score': analysis_data.get('avg_score', 0),
            'topics': analysis_data.get('topics', [])
        }

        # æ¸²æŸ“HTML
        html_content = template.render(**template_data)

        # ä¿å­˜æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        logger.info(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")

    def generate_markdown_summary(self, analysis_data: dict) -> str:
        """ç”ŸæˆMarkdownæ‘˜è¦"""
        topics = analysis_data.get('topics', [])

        markdown = f"""# å¾®åšçƒ­æœäº§å“åˆ›æ„åˆ†ææŠ¥å‘Š

## ğŸ“Š åˆ†ææ¦‚å†µ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| åˆ†ææ—¶é—´ | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} |
| åˆ†æè¯é¢˜æ•° | {analysis_data.get('total_topics', 0)} |
| ä¼˜ç§€åˆ›æ„ï¼ˆ80åˆ†+ï¼‰ | {analysis_data.get('excellent_count', 0)} |
| è‰¯å¥½åˆ›æ„ï¼ˆ60-79åˆ†ï¼‰ | {analysis_data.get('good_count', 0)} |
| å¹³å‡å¾—åˆ† | {analysis_data.get('avg_score', 0)} |

---

## ğŸ† Top 5 äº§å“åˆ›æ„

"""

        for i, topic in enumerate(topics[:5]):
            markdown += f"""
### {i+1}. {topic.get('product_name', '')} ({topic.get('score', 0)}åˆ†)

**ğŸ“° æ¥æºçƒ­æœ**: #{topic.get('rank', 0)} {topic.get('title', '')}

**ğŸ“‚ åˆ†ç±»**: {topic.get('category', '')}

**ğŸ¯ äº§å“å£å·**: "{topic.get('product_slogan', '')}"

**ğŸ’¡ æ ¸å¿ƒåŠŸèƒ½**: {topic.get('core_function', '')}

**ğŸ‘¥ ç›®æ ‡ç”¨æˆ·**: {topic.get('target_users', '')}

#### äº‹ä»¶èƒŒæ™¯
{topic.get('event_summary', '')}

#### å…³é”®è¦ç‚¹
"""
            for point in topic.get('key_points', []):
                markdown += f"- {point}\n"

            markdown += """
#### åŠŸèƒ½æ¸…å•
"""
            for feature in topic.get('feature_list', [])[:4]:
                markdown += f"- {feature}\n"

            markdown += f"""
#### ç”¨æˆ·ç—›ç‚¹
"""
            for pain in topic.get('user_pain_points', [])[:3]:
                markdown += f"- {pain}\n"

            markdown += f"""
#### è§£å†³æ–¹æ¡ˆ
{topic.get('solution', '')}

#### å•†ä¸šæ¨¡å¼
{topic.get('business_model', '')}

#### è¯„åˆ†è¯¦æƒ…
- æœ‰è¶£åº¦: {topic.get('interestingness', 0)}/80
- æœ‰ç”¨åº¦: {topic.get('usefulness', 0)}/20
- **æ€»åˆ†: {topic.get('score', 0)}/100**

---

"""

        markdown += """
## ğŸ“‹ å®Œæ•´è¯é¢˜åˆ—è¡¨

| æ’å | è¯é¢˜ | åˆ†ç±» | äº§å“åˆ›æ„ | å¾—åˆ† |
|------|------|------|----------|------|
"""
        for topic in topics:
            markdown += f"| #{topic.get('rank', 0)} | {topic.get('title', '')[:20]}... | {topic.get('category', '')} | {topic.get('product_name', '')} | {topic.get('score', 0)}åˆ† |\n"

        markdown += """

---

> æœ¬æŠ¥å‘Šç”±AIæ™ºèƒ½åˆ†æå¼•æ“è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚
> å®é™…äº§å“å¼€å‘éœ€è¿›è¡Œè¿›ä¸€æ­¥çš„å¸‚åœºè°ƒç ”ã€ç”¨æˆ·è®¿è°ˆå’Œå¯è¡Œæ€§åˆ†æã€‚
"""

        return markdown

    def run(self, topics_count: int = 20) -> dict:
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        # ç”Ÿæˆæ–‡ä»¶åå‰ç¼€ï¼ˆä½¿ç”¨YYMMDDæ ¼å¼ï¼Œå¦‚251222ï¼‰
        date_prefix = datetime.now().strftime('%y%m%d')
        date_str = date_prefix

        print("\n" + "=" * 60)
        print("ğŸš€ å¾®åšçƒ­æœäº§å“åˆ›æ„åˆ†æ v2.0")
        print("=" * 60 + "\n")

        # 1. è·å–çƒ­æœæ•°æ®
        print("ğŸ“¡ æ­¥éª¤1: è·å–å¾®åšçƒ­æœæ•°æ®...")
        hot_search_data = self.fetch_hot_search()

        if not hot_search_data:
            print("âŒ è·å–çƒ­æœæ•°æ®å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–APIå¯†é’¥")
            return None

        # ä¿å­˜åŸå§‹æ•°æ®
        data_file = os.path.join(self.base_dir, f'{date_str}_{self.output_prefix}_data.json')
        self.save_raw_data(hot_search_data, data_file)
        print(f"   âœ“ è·å–åˆ° {len(hot_search_data)} æ¡çƒ­æœæ•°æ®\n")

        # 2. æ™ºèƒ½åˆ†æ
        print(f"ğŸ” æ­¥éª¤2: æ™ºèƒ½åˆ†æçƒ­æœè¯é¢˜ï¼ˆåˆ†æå‰ {min(topics_count, len(hot_search_data))} æ¡ï¼‰...")
        analysis_results = self.analyzer.analyze_all(hot_search_data[:topics_count])

        # ä¿å­˜åˆ†æç»“æœ
        results_file = os.path.join(self.base_dir, f'{date_str}_{self.output_prefix}_results.json')
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        print(f"   âœ“ åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜\n")

        # 3. ç”ŸæˆHTMLæŠ¥å‘Š
        print("ğŸ“Š æ­¥éª¤3: ç”Ÿæˆå¯è§†åŒ–HTMLæŠ¥å‘Š...")
        html_file = os.path.join(self.base_dir, f'{date_str}_{self.output_prefix}_report.html')
        self.generate_html_report(analysis_results, html_file)
        print(f"   âœ“ HTMLæŠ¥å‘Š: {html_file}\n")

        # 4. ç”ŸæˆMarkdownæ‘˜è¦
        print("ğŸ“ æ­¥éª¤4: ç”ŸæˆMarkdownæ‘˜è¦æŠ¥å‘Š...")
        markdown_content = self.generate_markdown_summary(analysis_results)
        md_file = os.path.join(self.base_dir, f'{date_str}_{self.output_prefix}_summary.md')
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        print(f"   âœ“ Markdownæ‘˜è¦: {md_file}\n")

        # 5. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print("=" * 60)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("=" * 60)
        print(f"\nğŸ“ˆ åˆ†æç»Ÿè®¡:")
        print(f"   â€¢ åˆ†æè¯é¢˜æ•°: {analysis_results['total_topics']}")
        print(f"   â€¢ ä¼˜ç§€åˆ›æ„(80åˆ†+): {analysis_results['excellent_count']}")
        print(f"   â€¢ è‰¯å¥½åˆ›æ„(60-79åˆ†): {analysis_results['good_count']}")
        print(f"   â€¢ å¹³å‡å¾—åˆ†: {analysis_results['avg_score']}")

        print(f"\nğŸ† Top 5 äº§å“åˆ›æ„:")
        for i, topic in enumerate(analysis_results['topics'][:5]):
            print(f"   {i+1}. [{topic['score']}åˆ†] {topic['product_name']}")
            print(f"      æ¥æº: {topic['title'][:30]}...")
            print(f"      å£å·: {topic['product_slogan']}")

        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   â€¢ åŸå§‹æ•°æ®: {os.path.basename(data_file)}")
        print(f"   â€¢ åˆ†æç»“æœ: {os.path.basename(results_file)}")
        print(f"   â€¢ HTMLæŠ¥å‘Š: {os.path.basename(html_file)}")
        print(f"   â€¢ MDæ‘˜è¦: {os.path.basename(md_file)}")
        print()

        return analysis_results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¾®åšçƒ­æœäº§å“åˆ›æ„åˆ†æå·¥å…· v2.0')
    parser.add_argument('--api-key', help='å¤©è¡Œæ•°æ®APIå¯†é’¥')
    parser.add_argument('--output', default='weibo_analysis', help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    parser.add_argument('--topics', type=int, default=20, help='åˆ†æçš„è¯é¢˜æ•°é‡')

    args = parser.parse_args()

    # åˆ›å»ºæµç¨‹å®ä¾‹
    pipeline = WeiboHotSearchPipeline(
        api_key=args.api_key,
        output_prefix=args.output
    )

    # è¿è¡Œåˆ†æ
    pipeline.run(topics_count=args.topics)


if __name__ == '__main__':
    main()
